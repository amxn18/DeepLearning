## Sequence to Sequence Model with Encoder-Decoder Architecture: English to Hindi Translation

### ✅ Overview

This project demonstrates how to build a **Sequence-to-Sequence (Seq2Seq)** model for translating English sentences into Hindi using **Keras** and **TensorFlow**. It uses an **Encoder-Decoder LSTM architecture**. The model is trained on a dataset containing English-Hindi sentence pairs.

### ✅ Core Concepts Explained

#### 1️⃣ Sequence to Sequence (Seq2Seq) Learning

* Seq2Seq models handle tasks where both input and output are sequences.
* It involves **two LSTM networks**:

  * **Encoder:** Reads the input sentence and encodes it into a fixed-size context vector.
  * **Decoder:** Takes the context vector and generates the output sequence (translated sentence).

#### 2️⃣ Encoder-Decoder Architecture

* **Encoder:**

  * Processes the input sentence character by character.
  * Produces a context vector consisting of hidden states and cell states.
* **Decoder:**

  * Uses the context vector as its initial state.
  * Generates target sentences character by character until an end token (`\n`) is generated.

#### 3️⃣ One-Hot Encoding (Vectorization)

* Each character is represented as a one-hot vector.
* Shapes:

  * **encoderIpData:** `(number of samples, max encoder sequence length, number of unique input tokens)`
  * **decoderIpData:** `(number of samples, max decoder sequence length, number of unique output tokens)`
  * **decoderTargetData:** Same shape as `decoderIpData`, but shifted by one timestep.
* Helps neural networks handle categorical (character) data numerically.

#### 4️⃣ Special Tokens

* **`\t` (Tab):** Start-of-sequence marker for target text.
* **`\n` (Newline):** End-of-sequence marker for target text.
* These markers guide the decoder in knowing when to start and stop generating characters.

#### 5️⃣ Token Indexing

* Characters are assigned unique integer indices.
* Two dictionaries:

  * **ipTokenIdx:** Maps input characters to their indices.
  * **targetTokenIdx:** Maps target characters to their indices.

#### 6️⃣ Model Architecture

* **Encoder:**

  * LSTM layer outputs hidden and cell states.
  * Only the states (not the output sequence) are passed to the decoder.
* **Decoder:**

  * LSTM layer takes the encoder states as its initial state.
  * Dense layer with softmax activation outputs probabilities for each character.
* **Model:**

  * Trained to minimize categorical cross-entropy loss.

#### 7️⃣ Training Configuration

* **Batch Size:** 64
* **Epochs:** 100
* **Latent Dimension:** 256 (size of the hidden states in LSTM)
* **Validation Split:** 20% of the data.

#### 8️⃣ Inference Process (Prediction)

* After training, separate models are created for inference:

  * **Encoder Model:** Only outputs the states.
  * **Decoder Model:** Uses states and previous predicted characters to generate new characters.
* **Decoding Loop:**

  * Starts with the `\t` character.
  * Continues generating characters until `\n` is produced or max length is reached.

#### 9️⃣ Reverse Lookup Dictionaries

* **reverseIpCharIndex:** Maps indices back to input characters.
* **reverseTargetCharIndex:** Maps indices back to target characters.
* Helps in reconstructing readable text from one-hot encoded predictions.

### ✅ Key Points to Remember

* **No GPU needed after model training:** Inference can run on CPU since the model is already trained.
* **Model Deployment:** Model weights can be saved and loaded for later use.
* **Data Preprocessing:** Ensuring special tokens `\t` and `\n` are correctly handled avoids common errors.

### ✅ Project Steps Summary

1. **Data Preparation:** Reading English-Hindi sentence pairs.
2. **Tokenization & One-Hot Encoding:** Vectorizing the data.
3. **Model Building:** Defining encoder and decoder LSTM layers.
4. **Model Training:** Using categorical cross-entropy loss.
5. **Inference Models Creation:** Separating encoder and decoder for prediction.
6. **Translation Demo:** Feeding English sentences and retrieving Hindi translations.

### ✅ Advantages of Seq2Seq Approach

* Works for variable-length input and output sequences.
* Adaptable for multiple translation languages.
* Foundation for more advanced models like **Transformer-based architectures**.